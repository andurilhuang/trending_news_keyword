{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is a semi-manual tool to crawl and extract information through large amount of urls. There are parts you will need to manual input numbers based on the volumn of urls. The tool goes by a batch run format with threads to ensure crawl efficency.\n",
    "\n",
    "This tool also includes writing into the AWS Dynamo database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#install all needed packages\n",
    "import datetime\n",
    "import MySQLdb\n",
    "from langdetect import detect\n",
    "import requests\n",
    "from bs4 import UnicodeDammit\n",
    "from langdetect import detect\n",
    "from newspaper import Article\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tree import Tree\n",
    "from forex_python.converter import CurrencyCodes\n",
    "import threading\n",
    "import time\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "import boto3\n",
    "import decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global dictionary: Contractions\n",
    "Stored for text cleaning purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: get_contious_chunks\n",
    "\n",
    "NLTK Based rough NER tool for reference\n",
    "\n",
    "Input: text\n",
    "\n",
    "Input type: string\n",
    "\n",
    "Output: a list of NERs\n",
    "\n",
    "Output type: list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_continuous_chunks(text):\n",
    "    c = CurrencyCodes()\n",
    "    Clean_NER = []\n",
    "    NER = []\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "            current_chunk = list(set(current_chunk)) #remove duplicates\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "            else:\n",
    "                continue\n",
    "    NER = list(set(continuous_chunk)) #remove duplicates \n",
    "    for i in NER:\n",
    "        if c.get_symbol(i[:3]) is None:\n",
    "            Clean_NER.append(i)\n",
    "        \n",
    "        NER = Clean_NER \n",
    "\n",
    "    return NER  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: clean_text\n",
    "A rough clean for text mining purposes, *note there's a more established version in the actual keyword tool script. This one is only for reference*\n",
    "\n",
    "Input: text\n",
    "\n",
    "Input type: string\n",
    "\n",
    "Output: cleaned text\n",
    "\n",
    "Output type: string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):   \n",
    "    #c = CurrencyCodes()\n",
    "    #for i in text:\n",
    "    #    if c.get_symbol(i[:3]) is None:\n",
    "    #        i = \"\" #clear currency\n",
    "    for contraction in contractions.keys():\n",
    "        text = text.replace(contraction,contractions[contraction]) #all contraction to formal forms\n",
    "    cxo = ['CAO','CAIO','CIO','CFO','CTO','CEO','COO','CAE','CBO','CBDO','CCO','CDO','CEngO','CHRO','CMO','CNO','CPO','CRO','CVO']\n",
    "    for term in cxo:\n",
    "        text = text.replace(term, \"\") #clean text with CXO    \n",
    "    text = re.sub(r'http\\S+', '',text, flags = re.IGNORECASE) # remove in text urls with http\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE) # remove in text https\n",
    "    text = re.sub(r'www.\\S+', '',text, flags = re.IGNORECASE) #remove www urls\n",
    "    text = re.sub(r'\\bmobile:\\s*\\b', '',text, flags = re.IGNORECASE) #remove mobile\n",
    "    text = re.sub(r'\\bphone:\\s*\\b', '',text, flags = re.IGNORECASE) #remove phone\n",
    "    text = re.sub(r'\\be*-*mail:\\s*\\b', '',text, flags = re.IGNORECASE) #remove email\n",
    "    text = re.sub(r'\\S*tel:S*', '',text, flags = re.IGNORECASE) # remove number\n",
    "    text = re.sub(r'\\S*fax:S*', '',text, flags = re.IGNORECASE) # remove fax\n",
    "    text = re.sub(r'\\baddress:\\s*\\b', '',text, flags = re.IGNORECASE) #remove address\n",
    "    text = re.sub(r'\\bwebsite:\\s*\\b', '',text, flags = re.IGNORECASE) #remove image\n",
    "    text = re.sub(r'\\bimage:\\s*\\b', '',text, flags = re.IGNORECASE) #remove image\n",
    "    text = re.sub(r'\\S*@\\S*\\s?','',text, flags=re.MULTILINE) #remove in text email\n",
    "    text = re.sub(r'\\S*\\.com\\S*\\s?','',text, flags=re.MULTILINE) #remove .com urls\n",
    "    text = re.sub(r'\\S*\\.uk\\S*\\s?','',text, flags=re.MULTILINE) #remove .uk urls\n",
    "    text = text.replace('+',\"\")\n",
    "    text = text.replace('-',\"\")\n",
    "    text = text.replace('(',\"\")\n",
    "    text = text.replace(')',\"\")\n",
    "    text = text.replace('tel',\"\")\n",
    "    text = text.replace('Tel',\"\")\n",
    "    text = text.replace('TEL',\"\")\n",
    "    text = text.replace('CET',\"\")\n",
    "    text = text.replace('%',\"\")\n",
    "    text = text.replace('$',\"\")\n",
    "    text = text.replace('%',\"\")\n",
    "    text = text.replace('percent',\"\")\n",
    "    text = text.replace('£',\"\")\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: get_tier_3 (Manual script modification required for adjustments)\n",
    "Get relevant PB tier 3 database information. No input required. \n",
    "\n",
    "*Function returns only two lists of urls and time stamps*\n",
    "\n",
    "Manual modificaiton areas:\n",
    "date range\n",
    "\n",
    "please see in function notes for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tier_3():\n",
    "    \"\"\"\n",
    "    import all urls into a list\n",
    "    \"\"\"\n",
    "    import db_credit as dbc\n",
    "    db = MySQLdb.connect(host= dbc.host,  \n",
    "                         user= dbc.user,       \n",
    "                         passwd= dbc.passwd,\n",
    "                         port = dbc.port,\n",
    "                         db= dbc.db)\n",
    "     \n",
    "    # Create a Cursor object to execute queries.\n",
    "    cur = db.cursor()\n",
    "     \n",
    "    # Select data from table using SQL query.\n",
    "    #***NOTE: For date range modifications, user must manual change date range in the same formate below***\n",
    "\n",
    "    cur.execute(\n",
    "                \"\"\"\n",
    "                select a.url from tier_3_output.article a\n",
    "                where a.timestamp between '2017-01-26 00:00:00' and '2017-01-26 23:00:00'\n",
    "                \"\"\")    \n",
    "    url_list = []            \n",
    "    urls = list(cur.fetchall())\n",
    "    \n",
    "    for url in urls:\n",
    "        url_list.append(url[0])\n",
    "        \n",
    "    # Select data from table using SQL query.    \n",
    "    #***NOTE: For date range modifications, user must manual change date range in the same formate below***\n",
    "    cur.execute(\n",
    "                \"\"\"\n",
    "                select Date(a.timestamp) from tier_3_output.article a\n",
    "                where a.timestamp between '2017-01-26 00:00:00' and '2017-01-26 23:00:00'\n",
    "                \"\"\")    \n",
    "    time_list = []            \n",
    "    times = list(cur.fetchall())\n",
    "    \n",
    "    for time in times:\n",
    "        time_list.append(time[0])    \n",
    "        \n",
    "    print (\"DB info gathering completed\")\n",
    "    return url_list, time_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: get_url_info\n",
    "This is one of the cores of this scrip. The funciton crawl title and text from the news article\n",
    "\n",
    "Input: url\n",
    "\n",
    "Input type: string\n",
    "\n",
    "Output: title, text, url\n",
    "\n",
    "Output type: string(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_url_info(url):\n",
    "    try:\n",
    "        r = requests.head(url) \n",
    "        if r.status_code < 400: # if loads\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            if detect(article.title) == 'en': #English only\n",
    "                if len(article.text)>400: #filter out permission request\n",
    "                    title = UnicodeDammit(article.title).unicode_markup\n",
    "                    text = UnicodeDammit(article.text).unicode_markup\n",
    "                    #text = clean_text(text)\n",
    "                    test_url= url\n",
    "                    #NER = get_continuous_chunks(text)\n",
    "                    #timestamp = article.publish_date\n",
    "                    #print(\"success! \", datetime.datetime.now().time())\n",
    "                    return title, text, test_url#, NER #timestamp\n",
    "    except Exception as e:\n",
    "        print(\"fail \", datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: append_url_info\n",
    "*call back to get_url_info, the target for threading calls, append each url crawl result into lists*\n",
    "\n",
    "Input: url\n",
    "\n",
    "Input type: string\n",
    "\n",
    "Output: title_list, text_raw_list, url_list\n",
    "\n",
    "Output: list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_url_info(url):\n",
    "    try:\n",
    "        title, text, test_url = get_url_info(url)\n",
    "        if 'Please Sign In' not in text:\n",
    "            title_list.append(title)\n",
    "            text_raw_list.append(text)\n",
    "            test_urls.append(url)\n",
    "            #NER_list.append(NER)\n",
    "            #time_list.append(timestamp) \n",
    "        #elif not (NER[0] == 'Please Sign' or NER[0] == 'Javascript'):\n",
    "            #title_list.append(title)\n",
    "            #text_raw_list.append(text)\n",
    "            #test_urls.append(url)\n",
    "            #NER_list.append(NER)\n",
    "            #time_list.append(timestamp) \n",
    "            print (len(test_urls))\n",
    "        return title_list, text_raw_list, test_urls#, NER_list#, time_list\n",
    "    except (TypeError):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-funtion scripts starts here\n",
    "The flowing chunck get DB information and call back to get_tier_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below block to get all urls and timestamps from the time you hard-coded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB info gathering completed\n"
     ]
    }
   ],
   "source": [
    "url_list, date_list = get_tier_3()\n",
    "unique_date = set(date_list)    \n",
    "    \n",
    "date_url_dict = {}\n",
    "for date in unique_date:\n",
    "    date_url_dict[date] = []   \n",
    "    \n",
    "for date_key in date_url_dict.keys():\n",
    "    for i in range(len(date_list)):\n",
    "        if date_list[i] == date_key:\n",
    "            date_url_dict[date_key].append(url_list[i])\n",
    "            \n",
    "url_combine = []    \n",
    "for date_key in date_url_dict.keys():\n",
    "    urls = date_url_dict[date_key]\n",
    "    for url in urls:\n",
    "        url_combine.append(url)\n",
    "        \n",
    "times = date_list            \n",
    "times = [str(i) for i in times]\n",
    "diction = {}\n",
    "for i in range(len(url_list)):\n",
    "    diction[url_list[i]] = times[i]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The print tells you the size of data, for you to get an idea of how to slice that into batches. We reccomend slicing for any volun larger than 2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(url_combine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the list \"combine\" is a sliced list of the original \"url_combine\" in case the volumn is > 10k, below example is for slicing data >10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1820\n"
     ]
    }
   ],
   "source": [
    "combine = url_combine[:10000] #update this for data >10k to for example [10000:20000], and [20000:30000] after each run is finished\n",
    "print(len(combine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further slice into batches then combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = []\n",
    "#batches.append(combine) #use this and comment out the rest for data <1000\n",
    "batches.append(combine[:1000])\n",
    "batches.append(combine[1000:2000])\n",
    "batches.append(combine[2000:3000])\n",
    "batches.append(combine[3000:4000])\n",
    "batches.append(combine[4000:5000])\n",
    "batches.append(combine[5000:6000])\n",
    "batches.append(combine[6000:7000])\n",
    "batches.append(combine[7000:8000])\n",
    "batches.append(combine[8000:9000])\n",
    "batches.append(combine[9000:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DYNAMO DB writing scripts starts here\n",
    "For each batch int he batch list complibed above, the script writes data into the databse, for each batch it will return the total number of rows inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "fail  04:59:50.902274\n",
      "You must `download()` an article first!\n",
      "fail  04:59:54.188131\n",
      "You must `download()` an article first!\n",
      "fail  You must `download()` an article first!04:59:54.287196\n",
      "You must `download()` an article first!You must `download()` an article first!\n",
      "fail \n",
      "\n",
      "fail  fail  04:59:54.295203 04:59:54.299704\n",
      "\n",
      "04:59:54.308710\n",
      "You must `download()` an article first!\n",
      "You must `download()` an article first!fail You must `download()` an article first!You must `download()` an article first!You must `download()` an article first!You must `download()` an article first!You must `download()` an article first!You must `download()` an article first! \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "04:59:54.380759fail fail fail fail fail fail fail \n",
      "       04:59:54.43680904:59:54.41629504:59:54.39076504:59:54.41929804:59:54.42730304:59:54.42580204:59:54.439310\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You must `download()` an article first!\n",
      "fail  04:59:55.988369\n",
      "You must `download()` an article first!\n",
      "fail  04:59:56.112486\n",
      "You must `download()` an article first!\n",
      "fail  04:59:56.129996\n",
      "Article `download()` failed with 429 Client Error: Unknown for url: https://www.independent.ie/style/celebrity/celebrity-news/britney-spears-niece-wakes-up-from-coma-after-offroad-vehicle-accident-35433625.html on URL http://www.independent.ie/style/celebrity/celebrity-news/britney-spears-niece-wakes-up-from-coma-after-offroad-vehicle-accident-35433625.html\n",
      "fail  04:59:56.491251\n",
      "You must `download()` an article first!\n",
      "fail  04:59:56.549291\n",
      "Article `download()` failed with 429 Client Error: Unknown for url: https://www.independent.ie/irish-news/news/he-threatened-to-keep-our-deposits-if-we-didnt-write-a-letter-saying-we-were-happy-tenants-from-overcrowded-hell-house-35434501.html on URL http://www.independent.ie/irish-news/news/he-threatened-to-keep-our-deposits-if-we-didnt-write-a-letter-saying-we-were-happy-tenants-from-overcrowded-hell-house-35434501.html\n",
      "fail You must `download()` an article first! \n",
      "04:59:56.650358fail \n",
      " 04:59:56.652859\n",
      "You must `download()` an article first!\n",
      "You must `download()` an article first!fail  \n",
      "Article `download()` failed with 429 Client Error: Unknown for url: https://www.independent.ie/sport/soccer/arsenes-job-is-very-complicated-thierry-henrys-verdict-on-wengers-future-35433170.html on URL http://www.independent.ie/sport/soccer/arsenes-job-is-very-complicated-thierry-henrys-verdict-on-wengers-future-35433170.htmlArticle `download()` failed with 429 Client Error: Unknown for url: https://www.independent.ie/entertainment/music/one-directions-niall-horan-reveals-budget-backpacking-experience-with-family-35434268.html on URL http://www.independent.ie/style/celebrity/celebrity-news/one-directions-niall-horan-reveals-budget-backpacking-experience-with-family-35434268.html04:59:56.761932fail \n",
      "\n",
      "\n",
      " fail fail  04:59:56.758929 04:59:56.779944\n",
      "04:59:56.780944\n",
      "\n",
      "You must `download()` an article first!\n",
      "fail  04:59:57.513471\n",
      "You must `download()` an article first!\n",
      "fail  04:59:57.835725\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://economictimes.indiatimes.com/news/politics-and-nation/restrictions-after-clashes-between-nc-bjp/articleshow/57037673.cms on URL http://economictimes.indiatimes.com/news/politics-and-nation/restrictions-after-clashes-between-nc-bjp/articleshow/57037673.cms\n",
      "fail  04:59:58.146432\n",
      "You must `download()` an article first!\n",
      "fail  04:59:58.161442\n",
      "You must `download()` an article first!\n",
      "fail  04:59:58.204971\n",
      "2\n",
      "You must `download()` an article first!\n",
      "fail  04:59:58.397112\n",
      "You must `download()` an article first!\n",
      "fail  04:59:59.815115\n",
      "You must `download()` an article first!\n",
      "fail  You must `download()` an article first!04:59:59.918686\n",
      "\n",
      "fail  04:59:59.938198\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://www.geekwire.com/geekwire-picks/geekwire-deals-access-thousands-high-quality-icons-fonts-templates-fraction-cost/ on URL http://www.geekwire.com/geekwire-picks/geekwire-deals-access-thousands-high-quality-icons-fonts-templates-fraction-cost/\n",
      "fail  05:00:00.879841\n",
      "Article `download()` failed with 429 Client Error: Unknown for url: https://www.independent.ie/irish-news/eviction-will-turn-our-autistic-sons-world-upside-down-family-face-eviction-from-rental-home-35433258.html on URL http://www.independent.ie/irish-news/eviction-will-turn-our-autistic-sons-world-upside-down-family-face-eviction-from-rental-home-35433258.html\n",
      "fail  05:00:01.428707\n",
      "You must `download()` an article first!\n",
      "fail  05:00:02.016113\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://www.waterstechnology.com/inside-market-data/news/2481370/tullett-prebon-info-to-carry-equity-and-credit-research-analytics-from-valens on URL http://www.waterstechnology.com/inside-market-data/news/2481370/tullett-prebon-info-to-carry-equity-and-credit-research-analytics-from-valens\n",
      "fail  05:00:02.738094\n",
      "You must `download()` an article first!\n",
      "fail  05:00:03.323484\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://www.bvkap.de/events-medien/veranstaltungen/2018-01-01/showcase-veranstaltung-ginas-test on URL http://www.bvkap.de/events-medien/veranstaltungen/2018-01-01/showcase-veranstaltung-ginas-test\n",
      "fail  05:00:03.505606Article `download()` failed with 404 Client Error: Not Found for url: https://economictimes.indiatimes.com/news/international/world-news/brexit-bill-set-to-clear-major-parliamentary-hurdle/articleshow/57031415.cms on URL http://economictimes.indiatimes.com/news/international/world-news/brexit-bill-set-to-clear-major-parliamentary-hurdle/articleshow/57031415.cms\n",
      "\n",
      "Article `download()` failed with 429 Client Error: Unknown for url: https://www.independent.ie/world-news/syria-rejects-totally-untrue-amnesty-report-of-mass-hangings-in-prison-35433854.html on URL http://www.independent.ie/world-news/syria-rejects-totally-untrue-amnesty-report-of-mass-hangings-in-prison-35433854.htmlArticle `download()` failed with 429 Client Error: Unknown for url: https://www.independent.ie/style/celebrity/celebrity-news/nicole-and-keith-uncoupled-by-big-awards-date-clash-35433573.html on URL http://www.independent.ie/style/celebrity/celebrity-news/reese-and-tv-costars-are-my-pals-for-life-nicole-kidman-35433573.htmlfail \n",
      "\n",
      " fail fail 05:00:03.565146 Article `download()` failed with 429 Client Error: Unknown for url: https://www.independent.ie/irish-news/driver-fined-for-travelling-with-four-children-under-12-without-seatbelts-35433845.html on URL http://www.independent.ie/irish-news/driver-fined-for-travelling-with-four-children-under-12-without-seatbelts-35433845.htmlArticle `download()` failed with 429 Client Error: Unknown for url: https://www.independent.ie/irish-news/courts/man-avoids-fine-over-dog-fouling-due-to-syringes-left-in-city-park-35433624.html on URL http://www.independent.ie/irish-news/courts/man-avoids-fine-over-dog-fouling-due-to-syringes-left-in-city-park-35433624.html \n",
      "05:00:03.698234\n",
      "\n",
      "05:00:03.658208fail \n",
      "fail \n",
      "  05:00:03.989943\n",
      "05:00:04.049984\n",
      "You must `download()` an article first!\n",
      "fail  05:00:05.206765\n",
      "You must `download()` an article first!\n",
      "fail  You must `download()` an article first!05:00:05.291822\n",
      "\n",
      "fail  05:00:05.373376\n",
      "Article `download()` failed with 403 Client Error: Forbidden for url: https://www.institutionalinvestor.com/article/3660020/asset-management-hedge-funds-and-alternatives/hedge-fund-accused-of-defrauding-911-heroes.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+institutionalinvestor%2FdQyC+%28Institutional+Investor%3A+Asset+Management%29 on URL http://feedproxy.google.com/~r/institutionalinvestor/dQyC/~3/zbH7Bv5UkKU/hedge-fund-accused-of-defrauding-911-heroes.html\n",
      "fail  05:00:05.685095\n",
      "You must `download()` an article first!\n",
      "fail  05:00:06.087430\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://www.westfieldcorp.com/news/2017/02/08/wfd/asx_announcements/2997050.pdf on URL http://corporate.westfield.com/news/2017/02/08/wfd/asx_announcements/2997050.pdfArticle `download()` failed with 404 Client Error: Not Found for url: https://economictimes.indiatimes.com/news/politics-and-nation/just-spending-money-wont-clean-ganga-say-experts/articleshow/57035222.cms on URL http://economictimes.indiatimes.com/news/politics-and-nation/just-spending-money-wont-clean-ganga-say-experts/articleshow/57035222.cms\n",
      "\n",
      "fail fail   05:00:06.67755605:00:06.673054\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article `download()` failed with 403 Client Error: Forbidden for url: https://www.bizjournals.com/portland/news/2017/02/07/bill-to-boost-small-scale-renewables-in-oregon.html?ana=RSS%26s%3Darticle_search&utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+industry_5+%28Industry+Energy+%26+the+Environment%29 on URL http://feeds.bizjournals.com/~r/industry_5/~3/8-LyZzgP3PY/bill-to-boost-small-scale-renewables-in-oregon.html\n",
      "fail  05:00:07.136361\n",
      "You must `download()` an article first!Article `download()` failed with 403 Client Error: Forbidden for url: https://www.bizjournals.com/newyork/news/2017/02/07/meredith-taking-another-run-at-buying-time-inc.html?ana=RSS%26s%3Darticle_search&utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+industry_18+%28Industry+Media+%26+Marketing%29 on URL http://feeds.bizjournals.com/~r/industry_18/~3/UNscodkA9Jo/meredith-taking-another-run-at-buying-time-inc.html\n",
      "\n",
      "Article `download()` failed with 403 Client Error: Forbidden for url: https://www.bizjournals.com/seattle/news/2017/02/07/tmobile-binge-on-net-neutrality-fcc-investigation.html?ana=RSS%26s%3Darticle_search&utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+industry_7+%28Industry+Technology%29 on URL http://feeds.bizjournals.com/~r/industry_7/~3/Oh6Ia-5G2Ug/tmobile-binge-on-net-neutrality-fcc-investigation.htmlfail fail \n",
      "  fail 05:00:07.67372005:00:07.685227 \n",
      "\n",
      "05:00:07.818316\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://economictimes.indiatimes.com/news/defence/navys-rejection-of-tejas-is-a-lesson-failure-of-drdo/articleshow/57034043.cms on URL http://economictimes.indiatimes.com/news/defence/navys-rejection-of-tejas-is-a-lesson-failure-of-drdo/articleshow/57034043.cms\n",
      "fail  05:00:08.200570\n",
      "You must `download()` an article first!\n",
      "fail  05:00:08.654374\n",
      "3\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: https://www.smartcompany.com.au/startupsmart/advice/startupsmart-growth/young-entrepreneurs/things-things-this-founder-has-learnt-about-balancing-kids-with-growing-a-business/ on URL http://www.startupsmart.com.au/advice/growth/young-entrepreneurs/17-things-things-this-founder-has-learnt-about-balancing-kids-with-growing-a-business/\n",
      "fail  05:00:09.011112\n",
      "Article `download()` failed with 502 Server Error: Bad Gateway for url: https://investmentcentre.moneymanagement.com.au/news/712922/avenir-joins-fidante-stable- on URL http://www.moneymanagement.com.au/news/people-products/avenir-joins-fidante-stable\n",
      "fail  05:00:09.067649\n",
      "You must `download()` an article first!\n",
      "fail  05:00:09.629523\n",
      "You must `download()` an article first!\n",
      "fail  05:00:10.371017\n",
      "You must `download()` an article first!\n",
      "fail  05:00:11.468265\n",
      "4\n",
      "You must `download()` an article first!\n",
      "fail  05:00:15.155238\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "8687\n",
      "\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "127126\n",
      "\n",
      "128\n",
      "129\n",
      "130\n",
      "131132\n",
      "\n",
      "133\n",
      "fail  05:03:56.946525\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139140\n",
      "\n",
      "141\n",
      "142\n",
      "144143\n",
      "\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154155156\n",
      "\n",
      "\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "169168\n",
      "\n",
      "170\n",
      "171\n",
      "174172173\n",
      "\n",
      "\n",
      "175176\n",
      "177\n",
      "\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "240239\n",
      "\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256257258\n",
      "\n",
      "\n",
      "259\n",
      "260261\n",
      "\n",
      "262263\n",
      "\n",
      "264265\n",
      "\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270271\n",
      "\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278279\n",
      "\n",
      "280\n",
      "282281\n",
      "283\n",
      "\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298299\n",
      "\n",
      "300\n",
      "301302\n",
      "\n",
      "303304\n",
      "\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312313\n",
      "\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319320\n",
      "\n",
      "321322\n",
      "\n",
      "323\n",
      "324325\n",
      "\n",
      "326327\n",
      "\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333334\n",
      "\n",
      "335337336\n",
      "\n",
      "\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342343\n",
      "344\n",
      "\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349350\n",
      "\n",
      "352351\n",
      "\n",
      "353\n",
      "354\n",
      "355\n",
      "356357\n",
      "\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "368367\n",
      "\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373374375376\n",
      "\n",
      "\n",
      "\n",
      "377378\n",
      "\n",
      "379\n",
      "380\n",
      "381382\n",
      "\n",
      "383\n",
      "384385386387388\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "389\n",
      "390\n",
      "391\n",
      "392393\n",
      "\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "batch-run finished:  05:06:52.906777\n"
     ]
    }
   ],
   "source": [
    "for batch in batches:\n",
    "    title_list, text_raw_list, test_urls, NER_list, time_list = [], [], [], [], []\n",
    "\n",
    "    threads = [threading.Thread(target=append_url_info, args=(url,)) for url in batch] #url_combined in date sample method\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    text_clean_list = [clean_text(text) for text in text_raw_list]\n",
    "\n",
    "    NER_list = [get_continuous_chunks(title) for title in title_list]\n",
    "\n",
    "    for url in test_urls:\n",
    "        if url in diction.keys():\n",
    "            time_list.append(diction[url])\n",
    "\n",
    "    summary = {'url': test_urls, 'title': title_list, 'text_raw': text_raw_list, 'text_clean':text_clean_list, 'NER': NER_list, 'timestamp': time_list}\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "\n",
    "    #clean data\n",
    "    import collections\n",
    "    a = summary_df['title'].tolist()\n",
    "    dup_title = [item for item, count in collections.Counter(a).items() if count >1]\n",
    "    for title in summary_df['title']:\n",
    "        if title in dup_title:\n",
    "            summary_df = summary_df[summary_df['title'] != title]\n",
    "\n",
    "    #start writing data into DB        \n",
    "    obj = json.loads(summary_df.to_json(orient='records'))\n",
    "\n",
    "    dynamodb = boto3.resource('dynamodb', region_name='us-east-1')\n",
    "\n",
    "    table = dynamodb.Table('url_text')\n",
    "\n",
    "    for record in obj:\n",
    "        NER = record['NER']\n",
    "        text_clean = record['text_clean']\n",
    "        text_raw = record['text_raw']\n",
    "        timestamp = record['timestamp']\n",
    "        title = record['title']\n",
    "        url = record['url']\n",
    "\n",
    "        table.put_item(\n",
    "            Item={\n",
    "                    'NER': NER,\n",
    "                    'text_clean': text_clean,\n",
    "                    'text_raw': text_raw,\n",
    "                    'timestamp': timestamp,\n",
    "                    'title': title,\n",
    "                    'url': url,\n",
    "                }\n",
    "            )\n",
    "    print ('batch-run finished: ',datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
